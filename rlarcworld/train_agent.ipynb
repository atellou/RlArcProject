{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 10:31:03 - arc_batch_grid_env.py - INFO - Registering gymnasium environment\n"
     ]
    }
   ],
   "source": [
    "from torchrl.data.replay_buffers import (\n",
    "    TensorDictReplayBuffer,\n",
    "    LazyTensorStorage,\n",
    "    PrioritizedSampler,\n",
    ")\n",
    "from tensordict import TensorDict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from rlarcworld.arc_dataset import ArcDataset, ArcSampleTransformer\n",
    "from rlarcworld.enviroments.arc_batch_grid_env import ArcBatchGridEnv\n",
    "from rlarcworld.enviroments.wrappers.rewards import PixelAwareRewardWrapper\n",
    "from rlarcworld.agent.actor import ArcActorNetwork\n",
    "from rlarcworld.agent.critic import ArcCriticNetwork\n",
    "\n",
    "from rlarcworld.algorithms.d4pg import D4PG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 30\n",
    "color_values = 11\n",
    "batch_size=128\n",
    "max_steps = torch.randint(30, 100, size=(1,)).item()\n",
    "n_steps = torch.randint(3, 20 // 2, size=(1,)).item()\n",
    "gamma = 0.99\n",
    "env = ArcBatchGridEnv(grid_size, color_values, n_steps=n_steps, gamma=gamma)\n",
    "env = PixelAwareRewardWrapper(env, n_steps=n_steps, gamma=gamma)\n",
    "\n",
    "# Create an instance of the ArcDataset\n",
    "dataset = ArcDataset(\n",
    "    arc_dataset_dir=\"./dataset/training\",\n",
    "    keep_in_memory=True,\n",
    "    transform=ArcSampleTransformer(\n",
    "        (grid_size, grid_size), examples_stack_dim=10\n",
    "    ),\n",
    ")\n",
    "train_samples = DataLoader(dataset=dataset, batch_size=len(dataset) // 2)\n",
    "\n",
    "dataset_val = ArcDataset(\n",
    "    arc_dataset_dir=\"./dataset/evaluation\",\n",
    "    keep_in_memory=True,\n",
    "    transform=ArcSampleTransformer(\n",
    "        (grid_size, grid_size), examples_stack_dim=10\n",
    "    ),\n",
    ")\n",
    "val_samples = DataLoader(dataset=dataset, batch_size=len(dataset) // 2)\n",
    "replay_buffer = TensorDictReplayBuffer(\n",
    "    storage=LazyTensorStorage(batch_size),\n",
    "    sampler=PrioritizedSampler(\n",
    "        max_capacity=batch_size,\n",
    "        alpha=1.0,\n",
    "        beta=1.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "num_atoms = {\"pixel_wise\": 50, \"binary\": 3, \"n_reward\": 50 * n_steps}\n",
    "v_min = {\"pixel_wise\": -40, \"binary\": 0, \"n_reward\": -40 * n_steps}\n",
    "v_max = {\"pixel_wise\": 2, \"binary\": 1, \"n_reward\": 2 * n_steps}\n",
    "critic = ArcCriticNetwork(\n",
    "    size=grid_size,\n",
    "    color_values=color_values,\n",
    "    num_atoms=num_atoms,\n",
    "    v_min=v_min,\n",
    "    v_max=v_max,\n",
    ")\n",
    "\n",
    "actor = ArcActorNetwork(size=grid_size, color_values=color_values)\n",
    "d4pg = D4PG(\n",
    "    env=env,\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    train_samples=train_samples,\n",
    "    validation_samples=val_samples,\n",
    "    batch_size=batch_size,\n",
    "    replay_buffer=replay_buffer,\n",
    "    target_update_frequency=5,\n",
    "    n_steps=env.n_steps,\n",
    "    gamma=env.gamma,\n",
    "    tb_writer=SummaryWriter(log_dir=\"runs/test_validation_d4pg\"),\n",
    ")\n",
    "d4pg.fit(\n",
    "    max_steps=max_steps,\n",
    "    validation_steps_frequency=10,\n",
    "    validation_steps_per_train_step=10,\n",
    "    validation_steps_per_episode=max_steps,\n",
    "    logger_frequency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_nested_ref\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Invalid training reward history format - expected dict, got <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 23\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m     14\u001b[0m     ref[last_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray\n\u001b[1;32m     15\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid validation reward history format - expected np.ndarray for n_step, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mtype\u001b[39m(ref[last_key]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m ref, last_key \u001b[38;5;241m=\u001b[39m get_nested_ref(\n\u001b[1;32m     20\u001b[0m     d4pg\u001b[38;5;241m.\u001b[39mhistory, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/test_validation_d4pg/Train/Reward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m     24\u001b[0m     ref[last_key], \u001b[38;5;28mdict\u001b[39m\n\u001b[1;32m     25\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid training reward history format - expected dict, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mtype\u001b[39m(ref[last_key])\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m     29\u001b[0m     ref[last_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray\n\u001b[1;32m     30\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid training reward history format - expected np.ndarray for n_step, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mtype\u001b[39m(ref[last_key]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m     32\u001b[0m )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Invalid training reward history format - expected dict, got <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "assert os.path.isdir(\n",
    "    \"./runs/test_validation_d4pg\"\n",
    "), \"Directory 'runs/test_validation_d4pg' does not exist\"\n",
    "\n",
    "ref, last_key = get_nested_ref(\n",
    "    d4pg.history, \"Validation/Reward\"\n",
    ")\n",
    "assert isinstance(\n",
    "    ref[last_key], dict\n",
    "), \"Invalid validation reward history format - expected dict, got {}\".format(\n",
    "    type(ref[last_key])\n",
    ")\n",
    "assert isinstance(\n",
    "    ref[last_key][\"n_reward\"], np.ndarray\n",
    "), \"Invalid validation reward history format - expected np.ndarray for n_step, got {}\".format(\n",
    "    type(ref[last_key].get(\"n_step\", None))\n",
    ")\n",
    "\n",
    "ref, last_key = get_nested_ref(\n",
    "    d4pg.history, \"Train/Reward\"\n",
    ")\n",
    "\n",
    "assert isinstance(\n",
    "    ref[last_key], dict\n",
    "), \"Invalid training reward history format - expected dict, got {}\".format(\n",
    "    type(ref[last_key])\n",
    ")\n",
    "assert isinstance(\n",
    "    ref[last_key][\"n_reward\"], np.ndarray\n",
    "), \"Invalid training reward history format - expected np.ndarray for n_step, got {}\".format(\n",
    "    type(ref[last_key].get(\"n_step\", None))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlarcworld-ADz4tFzs-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
