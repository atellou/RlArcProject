{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.data.replay_buffers import (\n",
    "    TensorDictReplayBuffer,\n",
    "    LazyTensorStorage,\n",
    "    PrioritizedSampler,\n",
    ")\n",
    "from tensordict import TensorDict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from rlarcworld.arc_dataset import ArcDataset, ArcSampleTransformer\n",
    "from rlarcworld.enviroments.arc_batch_grid_env import ArcBatchGridEnv\n",
    "from rlarcworld.enviroments.wrappers.rewards import PixelAwareRewardWrapper\n",
    "from rlarcworld.agent.actor import ArcActorNetwork\n",
    "from rlarcworld.agent.critic import ArcCriticNetwork\n",
    "\n",
    "from rlarcworld.algorithms.d4pg import D4PG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 30\n",
    "color_values = 11\n",
    "batch_size=128\n",
    "max_steps = torch.randint(30, 100, size=(1,)).item()\n",
    "n_steps = torch.randint(3, 20 // 2, size=(1,)).item()\n",
    "gamma = 0.99\n",
    "env = ArcBatchGridEnv(grid_size, color_values, n_steps=n_steps, gamma=gamma)\n",
    "env = PixelAwareRewardWrapper(env, n_steps=n_steps, gamma=gamma)\n",
    "\n",
    "# Create an instance of the ArcDataset\n",
    "dataset = ArcDataset(\n",
    "    arc_dataset_dir=\"./dataset/training\",\n",
    "    keep_in_memory=False,\n",
    "    transform=ArcSampleTransformer(\n",
    "        (grid_size, grid_size), examples_stack_dim=10\n",
    "    ),\n",
    ")\n",
    "train_samples = DataLoader(dataset=dataset, batch_size=len(dataset) // 2)\n",
    "\n",
    "dataset_val = ArcDataset(\n",
    "    arc_dataset_dir=\"./dataset/evaluation\",\n",
    "    keep_in_memory=False,\n",
    "    transform=ArcSampleTransformer(\n",
    "        (grid_size, grid_size), examples_stack_dim=10\n",
    "    ),\n",
    ")\n",
    "val_samples = DataLoader(dataset=dataset, batch_size=len(dataset) // 2)\n",
    "replay_buffer = TensorDictReplayBuffer(\n",
    "    storage=LazyTensorStorage(batch_size),\n",
    "    sampler=PrioritizedSampler(\n",
    "        max_capacity=batch_size,\n",
    "        alpha=1.0,\n",
    "        beta=1.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "num_atoms = {\"pixel_wise\": 50, \"binary\": 3, \"n_reward\": 50 * n_steps}\n",
    "v_min = {\"pixel_wise\": -40, \"binary\": 0, \"n_reward\": -40 * n_steps}\n",
    "v_max = {\"pixel_wise\": 2, \"binary\": 1, \"n_reward\": 2 * n_steps}\n",
    "critic = ArcCriticNetwork(\n",
    "    size=grid_size,\n",
    "    color_values=color_values,\n",
    "    num_atoms=num_atoms,\n",
    "    v_min=v_min,\n",
    "    v_max=v_max,\n",
    ")\n",
    "\n",
    "actor = ArcActorNetwork(size=grid_size, color_values=color_values)\n",
    "d4pg = D4PG(\n",
    "    env=env,\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    train_samples=train_samples,\n",
    "    validation_samples=val_samples,\n",
    "    batch_size=batch_size,\n",
    "    replay_buffer=replay_buffer,\n",
    "    target_update_frequency=5,\n",
    "    n_steps=env.n_steps,\n",
    "    gamma=env.gamma,\n",
    "    tb_writer=SummaryWriter(log_dir=\"runs/test_validation_d4pg\"),\n",
    ")\n",
    "d4pg.fit(\n",
    "    max_steps=max_steps,\n",
    "    validation_steps_frequency=10,\n",
    "    validation_steps_per_train_step=10,\n",
    "    validation_steps_per_episode=max_steps,\n",
    "    logger_frequency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_nested_ref\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isdir(\n",
    "    \"./runs/test_validation_d4pg\"\n",
    "), \"Directory 'runs/test_validation_d4pg' does not exist\"\n",
    "\n",
    "ref, last_key = get_nested_ref(\n",
    "    d4pg.history, \"Validation/Reward\"\n",
    ")\n",
    "assert isinstance(\n",
    "    ref[last_key], dict\n",
    "), \"Invalid validation reward history format - expected dict, got {}\".format(\n",
    "    type(ref[last_key])\n",
    ")\n",
    "assert isinstance(\n",
    "    ref[last_key][\"n_reward\"], np.ndarray\n",
    "), \"Invalid validation reward history format - expected np.ndarray for n_step, got {}\".format(\n",
    "    type(ref[last_key].get(\"n_step\", None))\n",
    ")\n",
    "\n",
    "ref, last_key = get_nested_ref(\n",
    "    d4pg.history, \"Train/Reward\"\n",
    ")\n",
    "\n",
    "assert isinstance(\n",
    "    ref[last_key], dict\n",
    "), \"Invalid training reward history format - expected dict, got {}\".format(\n",
    "    type(ref[last_key])\n",
    ")\n",
    "assert isinstance(\n",
    "    ref[last_key][\"n_reward\"], np.ndarray\n",
    "), \"Invalid training reward history format - expected np.ndarray for n_step, got {}\".format(\n",
    "    type(ref[last_key].get(\"n_step\", None))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=2, size=(10, 1))\n",
    "print(x)\n",
    "print(torch.count_nonzero(x == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andresftu/.cache/pypoetry/virtualenvs/rlarcworld-ADz4tFzs-py3.13/lib/python3.13/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "transformer_model = torch.nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = transformer_model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10, 30, 30])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(20,10,2,30,30)[:,:, 0, :,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create an instance of the ArcDataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ArcDataset(\n\u001b[1;32m      3\u001b[0m     arc_dataset_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./dataset/training\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     keep_in_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     ),\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m train_samples \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m dataset_val \u001b[38;5;241m=\u001b[39m ArcDataset(\n\u001b[1;32m     12\u001b[0m     arc_dataset_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./dataset/evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     keep_in_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     ),\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m val_samples \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mdataset[:\u001b[38;5;241m20\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Create an instance of the ArcDataset\n",
    "dataset = ArcDataset(\n",
    "    arc_dataset_dir=\"./dataset/training\",\n",
    "    keep_in_memory=False,\n",
    "    transform=ArcSampleTransformer(\n",
    "        (30, 30), examples_stack_dim=10\n",
    "    ),\n",
    ")\n",
    "train_samples = DataLoader(dataset=dataset, batch_size=len(dataset) // 2)\n",
    "\n",
    "dataset_val = ArcDataset(\n",
    "    arc_dataset_dir=\"./dataset/evaluation\",\n",
    "    keep_in_memory=False,\n",
    "    transform=ArcSampleTransformer(\n",
    "        (30, 30), examples_stack_dim=10\n",
    "    ),\n",
    ")\n",
    "val_samples = DataLoader(dataset=dataset[:20], batch_size=len(dataset) // 2)\n",
    "replay_buffer = TensorDictReplayBuffer(\n",
    "    storage=LazyTensorStorage(100),\n",
    "    sampler=PrioritizedSampler(\n",
    "        max_capacity=1000,\n",
    "        alpha=1.0,\n",
    "        beta=1.0,\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/rlarcworld-ADz4tFzs-py3.13/lib/python3.13/site-packages/tensordict/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;34m\"\"\"Iterates over the first shape-dimension of the tensordict.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_dims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_samples\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rlarcworld-ADz4tFzs-py3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rlarcworld-ADz4tFzs-py3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rlarcworld-ADz4tFzs-py3.13/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rlarcworld-ADz4tFzs-py3.13/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rlarcworld-ADz4tFzs-py3.13/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    171\u001b[0m         {\n\u001b[1;32m    172\u001b[0m             key: collate(\n\u001b[1;32m    173\u001b[0m                 [d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map\n\u001b[1;32m    174\u001b[0m             )\n\u001b[0;32m--> 175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    176\u001b[0m         }\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "for data in val_samples:\n",
    "    data\n",
    "    break\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlarcworld-ADz4tFzs-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
